---
id: kullback-leibler-divergence
title: Kullback-Leibler Divergence
sidebar_label: Kullback-Leibler Divergence
---

## Introduction

KL divergence (also known as) relative entropy is a measure of the divergence between two probability distributions.

**! Help needed !**
