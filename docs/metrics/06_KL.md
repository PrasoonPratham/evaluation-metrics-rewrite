---
sidebar_position: 1
---

##### Kullback-Leibler Divergence

##### Introduction

KL divergence (also known as) relative entropy is a measure of the divergence between two probability distributions.


!Help needed!