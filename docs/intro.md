---
title: Evaluation Metrics
sidebar_label: Evaluation Metrics
sidebar_position: 1
---

### Why are we doing this?

We focus on evaluation metrics, because they do not have extensive dependencies on the intricacies of Artificial Intelligence, but only require the very basics of mathematical analysis and statistics to understand how they tick. They are afterall, only a formal way of measuring how well an answer corresponds to a question. With a focus on Evaluation Metrics, we become the gatekeepers of all the research activity that is conducted by the Research community in Artificial Intelligence.

Unfortunately, not many researchers still have deep intuitions behind many of the evaluation metrics we regularly use. We still treat them as just numbers that we have to optimize.

### How are we aiming to do this?

In this repository, we will focus on building a set of resources which will help anyone build a fundamental intuition around many of these evaluation metrics.

We will think about these evaluation metrics from the first principles, and ask questions like,

- why does this particular evaluation metric even need to question?
- What is its defining factor?
- For what classes of problems is this metric used?
- What are the common things we should be careful about when we are using this evaluation metric?

And So on and so forth.

### We are open! You can check out here!

We love people's support in growing and improving. We are open-source, and open to suggestions, issues, contributions and more!

If you're interested in helping us out, You can check us out [here](https://github.com/PrasoonPratham/evaluation-metrics-rewrite).
